{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28299ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f831798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyalex\n",
    "\n",
    "pyalex.config.email = \"krzysztof.d.pajak@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aae5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyalex import config\n",
    "\n",
    "config.max_retries = 1\n",
    "config.retry_backoff_factor = 0.1\n",
    "config.retry_http_codes = [429, 500, 503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932ec10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "sch = SemanticScholar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4caf193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_url = \"https://arxiv.org/abs/1706.03762\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f43e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000b3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = arxiv_url.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f46f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doi(ar_id: str) -> str:\n",
    "    try:    \n",
    "        paper = sch.get_paper(f\"ARXIV:{ar_id}\")\n",
    "\n",
    "        if paper and paper.externalIds and 'DOI' in paper.externalIds:\n",
    "            paper.externalIds['DOI']\n",
    "        else:\n",
    "            return f\"10.48550/arXiv.{ar_id}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return f\"10.48550/arXiv.{ar_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ad9c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = extract_doi(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a71643",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper =  sch.get_paper(f\"ARXIV:{arxiv_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bcabaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52a7d360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': '032274e57f7d8b456bd255fe76b909b2c1d7458e', 'externalIds': {'ArXiv': '1705.04304', 'DBLP': 'journals/corr/PaulusXS17', 'MAG': '2612675303', 'CorpusId': 21850704}, 'corpusId': 21850704, 'publicationVenue': {'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40', 'name': 'International Conference on Learning Representations', 'type': 'conference', 'alternate_names': ['Int Conf Learn Represent', 'ICLR'], 'url': 'https://iclr.cc/'}, 'url': 'https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e', 'title': 'A Deep Reinforced Model for Abstractive Summarization', 'venue': 'International Conference on Learning Representations', 'year': 2017, 'referenceCount': 43, 'citationCount': 1622, 'influentialCitationCount': 178, 'isOpenAccess': False, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/1705.04304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle'], 'publicationDate': '2017-05-11', 'journal': {'volume': 'abs/1705.04304', 'name': 'ArXiv'}, 'citationStyles': {'bibtex': '@Article{Paulus2017ADR,\\n author = {Romain Paulus and Caiming Xiong and R. Socher},\\n booktitle = {International Conference on Learning Representations},\\n journal = {ArXiv},\\n title = {A Deep Reinforced Model for Abstractive Summarization},\\n volume = {abs/1705.04304},\\n year = {2017}\\n}\\n'}, 'authors': [{'authorId': '2896063', 'name': 'Romain Paulus'}, {'authorId': '2228109', 'name': 'Caiming Xiong'}, {'authorId': '2166511', 'name': 'R. Socher'}], 'abstract': 'Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.references[0]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research-agent-backend)",
   "language": "python",
   "name": "research-agent-backend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
