{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4a0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import pdf_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "616f59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_url = \"https://arxiv.org/abs/2601.06953\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c4af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_content = await pdf_parser.download_pdf(arxiv_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31df235",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = pdf_parser.parse_pdf_to_markdown(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65481a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cbedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c3331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff87d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc48d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"},\n",
    "    ],\n",
    "    text_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b609adba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathReasoning(steps=[Step(explanation='Start with the equation 8x + 7 = -23.', output='8x + 7 = -23'), Step(explanation='Subtract 7 from both sides of the equation to isolate the term with x on one side.', output='8x + 7 - 7 = -23 - 7'), Step(explanation='Simplify both sides.', output='8x = -30'), Step(explanation='Divide both sides by 8 to solve for x.', output='x = -30/8'), Step(explanation='Simplify the fraction -30/8 by dividing both the numerator and the denominator by their greatest common divisor, which is 2.', output='x = -15/4'), Step(explanation='Convert the answer into a decimal, if needed.', output='x = -3.75')], final_answer='x = -15/4 \\\\, \\\\text{or} \\\\, -3.75')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are an expert research paper analyst. Please analyze the following research paper and provide a structured analysis.\n",
    "\n",
    "Extract the following information:\n",
    "\n",
    "1. **Paper Title**: The exact title of the paper\n",
    "\n",
    "2. **Summary**: A comprehensive summary organized into these sections:\n",
    "   - main_contribution: What is the key innovation or finding?\n",
    "   - methodology: What approach or methods were used?\n",
    "   - key_results: What were the main findings or outcomes?\n",
    "   - significance: Why is this work important?\n",
    "   - limitations: Any notable limitations or future work mentioned?\n",
    "\n",
    "3. **Benchmarks**: Extract ALL quantitative performance metrics and benchmark results mentioned in the paper. For each benchmark, provide:\n",
    "   - name: The name of the benchmark/dataset (e.g., \"ImageNet\", \"GLUE\", \"SQuAD\", \"COCO\")\n",
    "   - score: The numerical result achieved (e.g., \"88.5%\", \"76.3\", \"SOTA\")\n",
    "   - metric: The evaluation metric used (e.g., \"Accuracy\", \"F1-Score\", \"BLEU\", \"mAP\", \"Top-1 Accuracy\")\n",
    "\n",
    "Important: \n",
    "- Extract ALL benchmarks mentioned, including baseline comparisons\n",
    "- If no benchmarks are mentioned, return an empty list\n",
    "- Be precise with numerical values\n",
    "- Include the metric units\n",
    "\n",
    "Paper Content:\n",
    "{markdown_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6a0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f8042ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.parse(\n",
    "            model=\"gpt-5-mini\",            \n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert research paper analyst who provides clear, structured summaries and extracts quantitative benchmarks from academic papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            text_format=PaperAnalysis            \n",
    "        )\n",
    "        \n",
    "analysis = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ee4f633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132153"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5f78bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper proposes a fully synthetic pipeline (SynthSmith) to generate competition-level programming tasks, verified solutions, and high-quality test cases, and demonstrates that Code LLMs trained solely on these synthetic SFT and RL datasets (the X-Coder series) can achieve state-of-the-art competitive-programming performance without relying on real-world coding data.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.summary.main_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14923963",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\n",
    "            \"paper_title\": analysis.paper_title,\n",
    "            \"summary\": {\n",
    "                \"main_contribution\": analysis.summary.main_contribution,\n",
    "                \"methodology\": analysis.summary.methodology,\n",
    "                \"key_results\": analysis.summary.key_results,\n",
    "                \"significance\": analysis.summary.significance,\n",
    "                \"limitations\": analysis.summary.limitations\n",
    "            },\n",
    "            \"benchmarks\": [\n",
    "                {\n",
    "                    \"name\": b.name,\n",
    "                    \"score\": b.score,\n",
    "                    \"metric\": b.metric\n",
    "                }\n",
    "                for b in analysis.benchmarks\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13fb61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rich\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\venv\\lib\\site-packages (from rich) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mdurl, markdown-it-py, rich\n",
      "\n",
      "   -------------------------- ------------- 2/3 [rich]\n",
      "   -------------------------- ------------- 2/3 [rich]\n",
      "   ---------------------------------------- 3/3 [rich]\n",
      "\n",
      "Successfully installed markdown-it-py-4.0.0 mdurl-0.1.2 rich-14.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05b5c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper_title': 'X-Coder: Advancing Competitive Programming With Fully Synthetic Tasks, Solutions, And Tests', 'summary': {'main_contribution': 'The paper proposes a fully synthetic pipeline (SynthSmith) to generate competition-level programming tasks, verified solutions, and high-quality test cases, and demonstrates that Code LLMs trained solely on these synthetic SFT and RL datasets (the X-Coder series) can achieve state-of-the-art competitive-programming performance without relying on real-world coding data.', 'methodology': '1) Data synthesis: SynthSmith extracts and evolves competition-oriented features from existing code snippets, composes them via a two-stage feature-selection → task-formulation process, and supports multiple task styles (Codeforces / LeetCode / AtCoder). 2) Solution & test generation: multiple candidate solutions are sampled from strong teacher LLMs; test inputs are generated via prompting and a tool-based library (CYaRon); both solutions and tests are cross-validated using a dual-verification procedure (majority voting + weighted evaluation with hold-out validation) to select golden solutions and robust test suites. 3) Training recipe: supervised fine-tuning (SFT) on long chain-of-thought (long-CoT) demonstrations followed by reinforcement learning using a GRPO-like algorithm (code-execution-based reward = fraction of passed tests). 4) Evaluation and analysis: extensive ablations on data scaling, CoT length, task style, test-generation method, data-selection strategies, and failure modes.', 'key_results': '1) X-Coder models trained only on synthetic data achieve strong results on LiveCodeBench: X-Coder-Qwen2.5 (7B) reaches avg@8 = 62.9±1.8 on LiveCodeBench v5 and 55.8±1.9 on v6 after SFT+RL; X-Coder-Qwen3 (8B) reaches 64.0±2.5 (v5) and 56.5±1.3 (v6). 2) SFT-only X-Coder achieves avg@8 = 60.3±2.5 (Qwen2.5 backbone) and 59.4±2.0 (Qwen3 backbone). 3) SynthSmith synthetic SFT data outperform prior synthetic dataset baselines (e.g., +6.7 pts over OpenCodeReasoning; +4.6 pts over SelfCodeAlign on comparable token budgets). 4) Scaling SFT by increasing unique tasks is more effective than increasing solutions per task (example progression: 32k→200k tasks yields improvement from 43.7% → 62.7% on LCB v5). 5) Long-CoT solutions substantially outperform short-CoT (e.g., after 8 epochs: Long-CoT 60.3% (v5) vs Short-CoT 43.1%). 6) Tool-based test generation yields higher pass-rates on ground-truth solutions (87.9% vs 77.4%) and more discriminative tests (consensus 78.8% vs 82.0%).', 'significance': 'Demonstrates that high-quality, large-scale synthetic data (tasks, verified solutions, and test suites) can train competitive-programming-capable code LLMs that rival or exceed models trained on real-world data, reducing reliance on scarce or leakage-prone real benchmarks. Provides a practical pipeline (SynthSmith), dual-verification for quality, and empirical guidance on data scaling, CoT length, and staged SFT→RL training for building stronger code reasoners.', 'limitations': '1) Computational cost: full verification is expensive (verifying 200k samples required ∼1.6M long-CoT trajectories and 24M test executions), and SFT/RL training on long-CoT is compute-intensive. 2) Residual error/noise: dual-verification still yields nonzero error rates on real verified data (reported ~7.85% error rate of golden solutions on TACO-verified). 3) Some generated tasks can be ambiguous or unsolvable; even very strong proprietary models fail some tasks. 4) RL exhibits reward-hacking behaviors and can exploit test-case artifacts. 5) Current evaluation focuses on LiveCodeBench and certain standard code benchmarks; generalization to all coding domains requires further validation. 6) The approach depends on access to strong teacher models and a high-throughput execution infrastructure; democratizing this is non-trivial.'}, 'benchmarks': [{'name': 'LiveCodeBench v5', 'score': '62.9 ± 1.8', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen2.5 (SFT+RL, 7B, 40k synthetic RL examples)'}, {'name': 'LiveCodeBench v6', 'score': '55.8 ± 1.9', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen2.5 (SFT+RL, 7B)'}, {'name': 'LiveCodeBench v5', 'score': '64.0 ± 2.5', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen3 (SFT+RL, 8B, 40k synthetic RL examples)'}, {'name': 'LiveCodeBench v6', 'score': '56.5 ± 1.3', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen3 (SFT+RL, 8B)'}, {'name': 'LiveCodeBench v5', 'score': '60.3 ± 2.5', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen2.5-SFT (7B, SFT-only, 200k synthetic SFT samples)'}, {'name': 'LiveCodeBench v6', 'score': '53.5 ± 1.7', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen2.5-SFT'}, {'name': 'LiveCodeBench v5', 'score': '59.4 ± 2.0', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen3-SFT (8B, SFT-only)'}, {'name': 'LiveCodeBench v6', 'score': '55.4 ± 2.3', 'metric': 'avg@8 pass rate (%) — X-Coder-Qwen3-SFT'}, {'name': 'LiveCodeBench v5', 'score': '57.9', 'metric': 'pass@1 (%) — DeepCoder-Preview (14B, RL baseline, real data)'}, {'name': 'LiveCodeBench v6', 'score': '48.5', 'metric': 'pass@1 (%) — DeepCoder-Preview (14B)'}, {'name': 'LiveCodeBench v5', 'score': '58.1', 'metric': 'avg@32 (%) — AReal-boba² (14B, RL baseline)'}, {'name': 'LiveCodeBench v6', 'score': '56.7', 'metric': 'avg@32 (%) — AReal-boba² (14B)'}, {'name': 'LiveCodeBench v5', 'score': '47.6', 'metric': 'avg@32 (%) — Skywork-OR1 (7B, RL baseline)'}, {'name': 'LiveCodeBench v6', 'score': '40.0', 'metric': 'avg@32 (%) — Skywork-OR1 (7B)'}, {'name': 'LiveCodeBench v5', 'score': '51.3', 'metric': 'avg@64 (%) — OCR-Qwen-Instruct (7B, SFT baseline, real data)'}, {'name': 'LiveCodeBench v6', 'score': '44.5', 'metric': 'avg@64 (%) — OCR-Qwen-Instruct (7B)'}, {'name': 'LiveCodeBench v5', 'score': '57.3', 'metric': 'avg@16 (%) — rStar-Coder (7B, SFT baseline, mixed data)'}, {'name': 'LiveCodeBench v5', 'score': '58.5', 'metric': 'avg@8 (%) — Klear-Reasoner-SFT (8B, SFT baseline)'}, {'name': 'LiveCodeBench v6', 'score': '49.6', 'metric': 'avg@8 (%) — Klear-Reasoner-SFT (8B)'}, {'name': 'LiveCodeBench v5', 'score': '62.9', 'metric': 'avg@8 pass rate (%) — X-Coder-7B (reported in Figure 1 / aggregate)'}, {'name': 'LiveCodeBench v5 (Bespoke-Stratos)', 'score': '16.2', 'metric': 'pass@1 (%) — Bespoke-Stratos (7B, SFT baseline, 17k real tasks)'}, {'name': 'LiveCodeBench v6 (Bespoke-Stratos)', 'score': '8.57', 'metric': 'pass@1 (%) — Bespoke-Stratos'}, {'name': 'LiveCodeBench v5 (OpenThinker3)', 'score': '51.7', 'metric': 'Score (%) — OpenThinker3 (7B, SFT baseline, metric cell unspecified in table)'}, {'name': 'LiveCodeBench v6 (OpenThinker3)', 'score': '40.8', 'metric': 'Score (%) — OpenThinker3'}, {'name': 'LiveCodeBench v5 (OlympicCoder)', 'score': '40.9', 'metric': 'Score (%) — OlympicCoder (7B, SFT baseline, metric unspecified)'}, {'name': 'LiveCodeBench v6 (OlympicCoder)', 'score': '19.3', 'metric': 'Score (%) — OlympicCoder'}, {'name': 'LiveCodeBench v5 (Qwen3-8B)', 'score': '57.5', 'metric': 'Score (%) — Qwen3-8B (8B, backbone; metric unspecified in table)'}, {'name': 'LiveCodeBench v6 (Qwen3-8B)', 'score': '48.4', 'metric': 'Score (%) — Qwen3-8B'}, {'name': 'LiveCodeBench v5 (AceReason1.1, SFT-then-RL baseline)', 'score': '57.2', 'metric': 'avg@8 (%) — AceReason1.1 (7B)'}, {'name': 'LiveCodeBench v6 (AceReason1.1)', 'score': '52.1', 'metric': 'avg@8 (%) — AceReason1.1'}, {'name': 'LiveCodeBench v5 (MiMo)', 'score': '57.8', 'metric': 'avg@8 (%) — MiMo (7B, SFT→RL baseline)'}, {'name': 'LiveCodeBench v6 (MiMo)', 'score': '49.3', 'metric': 'avg@8 (%) — MiMo'}, {'name': 'OpenCodeReasoning (comparison)', 'score': '51.3', 'metric': 'avg@8 (%) — OCR-Qwen-7B-Instruct (baseline, LiveCodeBench v5)'}, {'name': 'OpenCodeReasoning (improved variant OCR-Qwen-Coder-7B-Instruct)', 'score': '53.6', 'metric': 'avg@8 (%) — OCR-Qwen-Coder-7B-Instruct (LiveCodeBench v5)'}, {'name': 'LiveCodeBench v5 — X-Coder vs OpenCodeReasoning', 'score': '60.3 (+6.7)', 'metric': 'avg@8 (%) — X-Coder-Qwen-Coder-7B-Instruct compared to OCR baseline (absolute diff)'}, {'name': 'LiveCodeBench v5 — SelfCodeAlign (10k synthetic samples)', 'score': '27.1', 'metric': \"Score (%) — SelfCodeAlign (10k synthetic) on LCB v5 (table reports 'Score')\"}, {'name': 'LiveCodeBench v5 — SynthSmith (10k synthetic samples)', 'score': '31.7', 'metric': 'Score (%) — SynthSmith (10k synthetic); +4.6 over SelfCodeAlign'}, {'name': 'SFT scaling (v1: 32k tasks ×1 solution) — LiveCodeBench v5', 'score': '43.7', 'metric': 'pass@1/avg@? (%) — reported SFT scaling performance (v1)'}, {'name': 'SFT scaling (v2: 64k×1) — LiveCodeBench v5', 'score': '47.0', 'metric': 'pass@? (%) — SFT scaling (v2)'}, {'name': 'SFT scaling (v3: 128k×1) — LiveCodeBench v5', 'score': '54.1', 'metric': 'pass@? (%) — SFT scaling (v3)'}, {'name': 'SFT scaling (v4: 200k×1) — LiveCodeBench v5', 'score': '62.7', 'metric': 'pass@? (%) — SFT scaling (v4)'}, {'name': 'Solution-scaling comparison (8K×8 vs 16K×4 vs 64K×1) — LiveCodeBench v5', 'score': 'Best scores shown ≈ 44.4, 45.1, 47.0', 'metric': 'best-achieved pass rate (%) — comparison indicating task-diversity scaling beats solution-per-task scaling'}, {'name': 'Long-CoT vs Short-CoT on LiveCodeBench v5 (epoch=3)', 'score': 'Long-CoT 42.9 ; Short-CoT 35.0', 'metric': 'pass rate (%) — LCB v5 after 3 epochs'}, {'name': 'Long-CoT vs Short-CoT on LiveCodeBench v5 (epoch=8)', 'score': 'Long-CoT 60.3 ; Short-CoT 43.1', 'metric': 'pass rate (%) — LCB v5 after 8 epochs (∆ ≈ +17.2 pts)'}, {'name': 'Long-CoT vs Short-CoT on LiveCodeBench v6 (epoch=3)', 'score': 'Long-CoT 36.0 ; Short-CoT 29.3', 'metric': 'pass rate (%) — LCB v6 after 3 epochs'}, {'name': 'Long-CoT vs Short-CoT on LiveCodeBench v6 (epoch=8)', 'score': 'Long-CoT 53.5 ; Short-CoT 37.6', 'metric': 'pass rate (%) — LCB v6 after 8 epochs (∆ ≈ +17.5 pts)'}, {'name': 'Prompting-based test generation — pass rate on ground-truth solutions', 'score': '77.4%', 'metric': 'Pass rate (%) — prompting-based test generation (avg tests 13.6, consensus 82.0%)'}, {'name': 'Tool-based test generation — pass rate on ground-truth solutions', 'score': '87.9%', 'metric': 'Pass rate (%) — tool-based (CYaRon) test generation (avg tests 18.3, consensus 78.8%)'}, {'name': 'Tool-based vs Prompting-based — consensus ratio', 'score': '78.8% (tool) vs 82.0% (prompting)', 'metric': 'Consensus ratio (%) — lower consensus indicates more discriminative tests'}, {'name': 'Failure-type distribution (16 rollouts on LiveCodeBench v5; 268 tasks) — Wrong Answer counts', 'score': 'Qwen2.5-Coder-7B-Instruct: 194.6 ± 10.7 ; Qwen3-8B: 87.1 ± 4.6 ; X-Coder-7B-SFT: 69.6 ± 3.7 ; X-Coder-7B: 67.9 ± 4.9', 'metric': 'mean count ± std — Wrong Answer occurrences across 16 rollouts (per-model)'}, {'name': 'Failure-type distribution (No Code Block) — LiveCodeBench v5 (16 rollouts)', 'score': 'Qwen2.5-Coder-7B-Instruct: 6.5 ± 8.2 ; Qwen3-8B: 7.7 ± 1.2 ; X-Coder-7B-SFT: 21.9 ± 3.7 ; X-Coder-7B: 11.8 ± 3.9', 'metric': 'mean count ± std — No Code Block occurrences'}, {'name': 'Failure-type distribution (Time Limit Exceeded) — LiveCodeBench v5 (16 rollouts)', 'score': 'Qwen2.5-Coder-7B-Instruct: 18.1 ± 4.1 ; Qwen3-8B: 21.8 ± 3.8 ; X-Coder-7B-SFT: 13.7 ± 3.3 ; X-Coder-7B: 11.5 ± 2.6', 'metric': 'mean count ± std — TLE occurrences'}, {'name': 'Failure-type distribution (Syntax Error) — LiveCodeBench v5 (16 rollouts)', 'score': 'X-Coder-7B: 8.3 ± 2.2 (others reported 0.0)', 'metric': 'mean count ± std — Syntax Error occurrences'}, {'name': 'Pass rate by reasoning token length — aggregated bins (LiveCodeBench v5)', 'score': '0–5k: 100% ; 5–10k: 92.7% ; 10–15k: 78% ; 15–20k: 69.2% ; 20–25k: 41.7% ; >25k: 16.7%', 'metric': 'Pass rate (%) — aggregated by reasoning-token-length bins (shows sharp decline with longer reasoning traces)'}, {'name': 'Pass rate by problem difficulty (overall, LiveCodeBench v5; 268 tasks)', 'score': 'Easy 96.8% (61/63) ; Medium 73.3% (63/86) ; Hard 37.8% (45/119)', 'metric': 'pass rate (%) and raw counts — aggregated by difficulty'}, {'name': 'Test-output labeling accuracy via voting (n candidate solutions)', 'score': 'n=4: 94.39% ; n=8: 94.73% ; n=16: 95.13%', 'metric': 'labeling accuracy (%) — majority-vote labeling of test outputs (TACO-verified)'}, {'name': 'Dual-verification golden-solution error rate on TACO-verified (n=8)', 'score': '7.85%', 'metric': 'error rate (%) — fraction of golden solutions that fail ground-truth tests (n=8 candidate solutions)'}, {'name': 'Distribution of golden-solution pass rates on voted test cases (R1-0528)', 'score': 'Ranges: (0,20): 13.12% ; [20,40): 17.29% ; [40,60): 17.57% ; [60,80): 14.94% ; [80,100): 13.39% ; 100: 23.66%', 'metric': 'task fraction (%) — distribution of pass-rate ranges for golden solutions (synthetic tasks)'}, {'name': 'Proprietary LLMs first-try pass-rate distribution on test cases (Qwen3-Max / Gemini2.5-pro / GPT5-High)', 'score': 'Example: 100% pass: Qwen3-Max 23.16% ; Gemini2.5-pro 28.18% ; GPT5-High 66.98% (other range breakdowns reported)', 'metric': 'fraction (%) of tasks achieving first-try perfect pass rate on voted test cases'}, {'name': 'Verification results on TACO-verified dataset (varying n candidate solutions)', 'score': 'n=4: avg pass rate 91.79% (test-case level), full pass (task-level) 84.20% ; n=8: 92.15% / 85.00% ; n=16: 92.50% / 85.80%', 'metric': 'pass rates (%) — test-case-level and task-level success for dual-verification'}, {'name': 'Solvability proxy: pass@1 of proprietary LLMs on voted test cases (distribution)', 'score': 'R1-0528: 100% pass on 23.66% of tasks ; Qwen3-Max: 23.16% ; Gemini2.5-Pro: 28.18% ; GPT5-High: 66.98%', 'metric': 'fraction (%) of tasks with perfect pass@1 by model'}, {'name': 'LiveCodeBench v5 — Llama-3.1-8B-Instruct family', 'score': 'Llama-3.1-8B-Instruct: 11.8 ; FuseChat-Llama-3.1-8B-Instruct: 12.6 ; X-Coder-Llama3.1-8B-SFT-32k-sample: 25.2 ; X-Coder-Llama3.1-8B-SFT+RL-10k-sample: 27.1', 'metric': 'v5 Score (%) — demonstrates generality across model families (improvement after SFT and SFT+RL)'}, {'name': 'HumanEval / MBPP family (HE, HE+, MBPP, MBPP+, Avg) — Qwen2.5-Coder-7B-Instruct', 'score': 'HE 88.4 ; HE+ 84.1 ; MBPP 83.5 ; MBPP+ 71.7 ; Avg 81.9', 'metric': 'accuracy (%) — standard code generation benchmarks'}, {'name': 'HumanEval / MBPP family — X-Coder-7B-SFT', 'score': 'HE 89.6 ; HE+ 84.8 ; MBPP 88.9 ; MBPP+ 73.5 ; Avg 84.2', 'metric': 'accuracy (%)'}, {'name': 'HumanEval / MBPP family — X-Coder-7B (SFT+RL)', 'score': 'HE 89.6 ; HE+ 84.1 ; MBPP 89.2 ; MBPP+ 75.7 ; Avg 84.7', 'metric': 'accuracy (%)'}, {'name': 'LiveCodeBench v2 → v5 (data-leakage style comparison)', 'score': 'Qwen3-8B: v2 88.1 → v5 57.5 (∆ -30.6) ; X-Coder-7B-SFT: 78.2 → 60.3 (∆ -17.9) ; X-Coder-7B: 80.1 → 62.9 (∆ -17.2)', 'metric': 'pass rate (%) and delta — performance drop from older (possibly leaked) to recent benchmark; indicates lower leakage risk for synthetic-trained models'}]}\n"
     ]
    }
   ],
   "source": [
    "print(analysis_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research-agent-backend)",
   "language": "python",
   "name": "research-agent-backend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
